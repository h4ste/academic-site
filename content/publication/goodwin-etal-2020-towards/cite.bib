@inproceedings{goodwin-etal-2020-towards,
 abstract = {Automatic summarization research has traditionally focused on providing high quality general-purpose summaries of documents. However, there are many applications which require more specific summaries, such as supporting question answering or topic-based literature discovery. In this paper we study the problem of conditional summarization in which content selection and surface realization are explicitly conditioned on an ad-hoc natural language question or topic description. Because of the difficulty in obtaining sufficient reference summaries to support arbitrary conditional summarization, we explore the use of multi-task fine-tuning (MTFT) on twenty-one natural language tasks to enable zero-shot conditional summarization on five tasks. We present four new summarization datasets, two novel ``onlineâ€³ or adaptive task-mixing strategies, and report zero-shot performance using T5 and BART, demonstrating that MTFT can improve zero-shot summarization quality.},
 address = {Online},
 author = {Goodwin, Travis  and
Savery, Max  and
Demner-Fushman, Dina},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 doi = {10.18653/v1/2020.findings-emnlp.289},
 month = {November},
 pages = {3215--3226},
 publisher = {Association for Computational Linguistics},
 title = {Towards Zero-Shot Conditional Summarization with Adaptive Multi-Task Fine-Tuning},
 url = {https://www.aclweb.org/anthology/2020.findings-emnlp.289},
 year = {2020}
}

